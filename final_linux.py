def compute(img_path):
    # -*- coding: utf-8 -*-
    """FaceRecognition_Draft1.ipynb

    Automatically generated by Colab.

    Original file is located at
        https://colab.research.google.com/drive/1_2ZY22yOVYMeUvbc3KnVMEvsTyiKPTCc
    """

    import numpy as np
    import pickle
    from sklearn.svm import SVC
    from sklearn.decomposition import PCA
    from sklearn.model_selection import train_test_split
    from sklearn.decomposition import PCA
    from sklearn.svm import SVC
    from tensorflow.keras.applications import VGG19
    from tensorflow.keras.applications.vgg19 import preprocess_input
    from tensorflow.keras.preprocessing import image

    #Here we have called vgg16 for its implementation before extraction .
    # Got to know about it from this blog
    # link:- https://franky07724-57962.medium.com/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1#:~:text=Using%20Keras'%20Pre%2Dtrained%20Models%20for%20Feature%20Extraction%20in%20Image%20Clustering,-franky&text=Keras%20provides%20a%20set%20of,feature%20extraction%2C%20and%20transfer%20learning.

    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(240,240, 3)) # Using 3 for dimensional aspect

    #References used to work with vgg16
    #We used img_to_array func from keras library, because there was a shape issue happening during extraction
    #link:- https://www.tensorflow.org/api_docs/python/tf/keras/utils/img_to_array
    #Used this blog for code and pre-processing implementation
    #link: https://franky07724-57962.medium.com/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1#:~:text=Using%20Keras'%20Pre%2Dtrained%20Models%20for%20Feature%20Extraction%20in%20Image%20Clustering,-franky&text=Keras%20provides%20a%20set%20of,feature%20extraction%2C%20and%20transfer%20learning.

    def convo_features(img_path):
        img = image.load_img(img_path, target_size=(240, 240))

        # Display the resized image
        # plt.imshow(img)
        # plt.title('Resized Image')
        # plt.show()

        # Convert the image to an array and expand dimensions for model input
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis=0)
        x = preprocess_input(x)

        # Predict using the base model
        features = base_model.predict(x)
        return features.flatten()


    # Load the combined model
    import joblib
    loaded_model_components = joblib.load('Final_model.pkl')
    loaded_pca = loaded_model_components['pca']
    loaded_svm = loaded_model_components['svm']


    # Function to preprocess image and extract features
    def preprocess_and_predict(image_path, loaded_pca, loaded_svm):
        # Preprocessing the image
        test_features = convo_features(image_path)
        pca_features = loaded_pca.transform(test_features.reshape(1, -1))

        # Predicting the class using the SVM model
        predicted_class = loaded_svm.predict(pca_features)
        return predicted_class

    # Test the model with a new image
    test_image_path = img_path
    predicted_class = preprocess_and_predict(test_image_path, loaded_pca, loaded_svm)
    print("Predicted Person: ", predicted_class)

    print(type(str(predicted_class[0])))

    from gtts import gTTS

    # This module is imported so that we can
    # play the converted audio
    import os

    mytext = f"{predicted_class[0]} is in front of you"
    # mytext = "Harshit"
    language = 'en'

    myobj = gTTS(text=mytext, lang=language, slow=False)

    myobj.save("face_recog.mp3")
    os.system("mpg321 face_recog.mp3")

    # Install Tesseract OCR and Python wrapper

    # !apt update
    # !apt install -y tesseract-ocr
    # !pip install pytesseract

    import pytesseract
    from PIL import Image
    # import matplotlib.pyplot as plt
    # import os

    pytesseract.pytesseract.tesseract_cmd = r"/usr/bin/tesseract"

    def display_text(img):

        # plt.imshow(img)
        # plt.axis('off')  # No axes for cleaner presentation
        # plt.show()

        # Perform OCR using Tesseract
        text = pytesseract.image_to_string(img, lang='eng')

        # Print the extracted text
        print("Extracted Text:")
        print(text)
        return text

    # image_dir = '/kaggle/input/test-images'

    # List all files in the directory and process each image
    # image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith(('png', 'jpg', 'jpeg'))]
    # for file in image_files:
        # display_text(file)
    img = Image.open(img_path)
    textocr = display_text(img)


    # mytext = "Harshit"
    language = 'en'

    myobj = gTTS(text=textocr, lang=language, slow=False)

    myobj.save("ocr_audio.mp3")














import time
from google.cloud import storage

def upload(bucket_name, src, dest_blob):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(dest_blob)
    blob.upload_from_filename(src)
    print(f"Uploaded {src} to {dest_blob}")

def download(bucket_name, src_blob, dest_file):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(src_blob)
    blob.download_to_filename(dest_file)
    print(f"Downloaded {src_blob} to {dest_file}")

def process_and_upload(bucket_name, src_blob, dest_blob):
    temp_file = "image.jpg"
    download(bucket_name, src_blob, temp_file)
    compute(temp_file)
    upload(bucket_name, "face_recog.mp3", "audio/face_recog.mp3")
    upload(bucket_name, "ocr_audio.mp3", "audio/ocr_audio.mp3")

if __name__ == "__main__":
    while True:
        try:
            s = time.time()
            process_and_upload("chakshu-anveshan", "image.jpg", "audio")
            e = time.time()
            print(f"Time taken: {e-s}")
            time.sleep(2)
        except Exception as e:
            print("An error occurred:", e)